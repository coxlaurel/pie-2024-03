<!DOCTYPE html>
<html style="font-size: 16px;" lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="​Electrician Services, ​Satisfaction Guaranteed, Our Services, Testimonials, ​Feel Free To Ask Me Anything">
    <meta name="description" content="">
    <title>Software</title>
    <link rel="stylesheet" href="nicepage.css" media="screen">
<link rel="stylesheet" href="Software.css" media="screen">
    <script class="u-script" type="text/javascript" src="jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 7.1.0, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    <link id="u-page-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i,900,900i|Montserrat:100,100i,200,200i,300,300i,400,400i,500,500i,600,600i,700,700i,800,800i,900,900i|Arizonia:400">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": ""
}</script>
    <meta name="theme-color" content="#478ac9">
    <meta property="og:title" content="Software">
    <meta property="og:type" content="website">
  <meta data-intl-tel-input-cdn-path="intlTelInput/"></head>
  <body data-path-to-root="./" data-include-products="false" class="u-body u-xl-mode" data-lang="en"><header class="u-clearfix u-header u-header" id="sec-4e5f"><div class="u-clearfix u-sheet u-sheet-1">
        <img class="u-image u-image-contain u-image-default u-image-1" src="images/image1.png" alt="" data-image-width="558" data-image-height="418" data-href="./">
        <nav class="u-menu u-menu-dropdown u-offcanvas u-menu-1">
          <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px;">
            <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-top-bottom-menu-spacing u-hamburger-link u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#">
              <svg class="u-svg-link" viewBox="0 0 24 24"><use xlink:href="#menu-hamburger"></use></svg>
              <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
            </a>
          </div>
          <div class="u-custom-menu u-nav-container">
            <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="./" style="padding: 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" style="padding: 10px 20px;">Sprints!</a><div class="u-nav-popup"><ul class="u-h-spacing-20 u-nav u-unstyled u-v-spacing-10"><li class="u-nav-item"><a class="u-button-style u-nav-link u-white" href="Sprint-1.html">Sprint 1</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-white" href="Sprint2.html">Sprint 2</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-white" href="Sprint-3.html">Sprint 3</a>
</li></ul>
</div>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" style="padding: 10px 20px;">Systems</a><div class="u-nav-popup"><ul class="u-h-spacing-20 u-nav u-unstyled u-v-spacing-10"><li class="u-nav-item"><a class="u-button-style u-nav-link u-white" href="Mechanical.html">Mechanical</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-white" href="Electrical.html">Electrical</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-white" href="Software.html">Software</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-white" href="Firmware.html">Firmware</a>
</li></ul>
</div>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="Bill-of-Materials.html" style="padding: 10px 20px;">Bill of Materials</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="About-Us.html" style="padding: 10px 20px;">About Us</a>
</li></ul>
          </div>
          <div class="u-custom-menu u-nav-container-collapse">
            <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
              <div class="u-inner-container-layout u-sidenav-overflow">
                <div class="u-menu-close"></div>
                <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-4"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="./">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link">Sprints!</a><div class="u-nav-popup"><ul class="u-h-spacing-20 u-nav u-unstyled u-v-spacing-10"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Sprint-1.html">Sprint 1</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Sprint2.html">Sprint 2</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Sprint-3.html">Sprint 3</a>
</li></ul>
</div>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link">Systems</a><div class="u-nav-popup"><ul class="u-h-spacing-20 u-nav u-unstyled u-v-spacing-10"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Mechanical.html">Mechanical</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Electrical.html">Electrical</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Software.html">Software</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Firmware.html">Firmware</a>
</li></ul>
</div>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Bill-of-Materials.html">Bill of Materials</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="About-Us.html">About Us</a>
</li></ul>
              </div>
            </div>
            <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
          </div>
        </nav>
        <p class="u-custom-font u-text u-text-default u-text-1">Cyberflex</p>
      </div></header>
    <section class="u-clearfix u-image u-shading u-section-1" id="sec-0876" data-image-width="720" data-image-height="720">
      <div class="u-clearfix u-sheet u-valign-middle-lg u-valign-middle-md u-valign-middle-sm u-valign-middle-xl u-sheet-1">
        <h1 class="u-text u-text-1" data-animation-name="customAnimationIn" data-animation-duration="2000">Software</h1>
        <p class="u-text u-text-2">Our software team was very intentional in achieving their learning goals of understanding signal processing, optimizing machine learning models, predicting signals in real time, and creating user friendly data collection systems.&nbsp;</p>
      </div>
    </section>
    <section class="u-clearfix u-section-2" id="sec-6ab8">
      <div class="u-clearfix u-sheet u-sheet-1">
        <div class="data-layout-selected u-clearfix u-expanded-width u-layout-wrap u-layout-wrap-1">
          <div class="u-layout" style="">
            <div class="u-layout-row" style="">
              <div class="u-align-center u-container-align-center u-container-style u-layout-cell u-size-30 u-layout-cell-1">
                <div class="u-container-layout u-valign-middle u-container-layout-1">
                  <h2 class="u-custom-font u-font-merriweather u-text u-text-default u-text-1" data-animation-name="customAnimationIn" data-animation-duration="2000">What We Accomplished!</h2>
                </div>
              </div>
              <div class="u-container-style u-layout-cell u-size-30 u-layout-cell-2">
                <div class="u-container-layout u-container-layout-2">
                  <ul class="u-custom-list u-spacing-10 u-text u-text-default u-text-2">
                    <li style="padding-left: 20px;">
                      <div class="u-list-icon u-text-palette-1-light-1">
                        <div style="font-size: 1.2em; margin: -1.2em;">►</div>
                      </div>
                    </li>
                    <li style="padding-left: 20px;"> Learned to process sEMG signals</li>
                    <li style="padding-left: 20px;">
                      <div class="u-list-icon u-text-palette-1-light-1">
                        <div style="font-size: 1.2em; margin: -1.2em;">►</div>
                      </div>Trained and Optimized Machine Learning (CNN) models
                    </li>
                    <li style="padding-left: 20px;">
                      <div class="u-list-icon u-text-palette-1-light-1">
                        <div style="font-size: 1.2em; margin: -1.2em;">►</div>
                      </div>
                    </li>
                    <li style="padding-left: 20px;">
                      <div class="u-list-icon u-text-palette-1-light-1">
                        <div style="font-size: 1.2em; margin: -1.2em;">►</div>
                      </div>Optimized an sEMG data collection system for multiple sensors
                    </li>
                    <li style="padding-left: 20px;">
                      <div class="u-list-icon u-text-palette-1-light-1">
                        <div style="font-size: 1.2em; margin: -1.2em;">►</div>
                      </div>
                      <div class="u-list-icon u-text-palette-1-light-1">
                        <div style="font-size: 1.2em; margin: -1.2em;">►</div>
                      </div>Successfully implemented real time recognition of gestures
                    </li>
                  </ul>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="u-clearfix u-section-3" id="sec-aa7e">
      <div class="u-clearfix u-sheet u-sheet-1">
        <div class="custom-expanded data-layout-selected u-clearfix u-gutter-10 u-layout-wrap u-layout-wrap-1">
          <div class="u-layout">
            <div class="u-layout-row">
              <div class="u-size-60">
                <div class="u-layout-col">
                  <div class="u-align-left u-border-no-bottom u-border-no-left u-border-no-right u-border-no-top u-container-align-left u-container-style u-layout-cell u-right-cell u-size-60 u-layout-cell-1">
                    <div class="u-container-layout u-container-layout-1">
                      <h2 class="u-align-left u-custom-font u-font-merriweather u-text u-text-1">Signal Processing<br>
                        <span style="font-size: 1.125rem;"> One of our learning goals was to determine how sEMG signals are processed to be properly used in our context. The following was our approach to this:&nbsp;</span>
                        <br>
                      </h2>
                      <p class="u-align-left u-text u-text-2"> This project involves two distinct pipelines for processing sEMG data to control the prosthetic arm: raw and pre-processed signals. Although the sensors we used to detect EMG signals have pins which output the filtered and raw signal respectively, we decided it would still be beneficial to learn how the signals from the emg are processed. Thus we created two pipelines, one designed to handle raw sEMG signals directly from the sensors and one assuming that the sEMG data has already been pre-processed. <br>
                        <br>The first pipeline is designed to handle raw sEMG signals directly from the sensors. This pipeline runs an FFT (Fast Fourier Transform) on the raw signals, processing the raw signals through several stages, including filtering, rectification, and normalization. It begins by applying a bandpass filter to isolate the relevant frequency range and remove unwanted noise and artifacts. The signal is then rectified to ensure it only has positive values, followed by the application of a moving RMS envelope to smooth the signal. Finally, the processed signal is normalized against maximum voluntary contraction (MVC) values to standardize the data for consistent use across different users. This pipeline allows real-time control of the prosthetic arm by interpreting the muscle signals from scratch. <br>
                        <br>The second pipeline assumes that the sEMG data has already been pre-processed. In this case, the raw signal has been filtered, rectified, and normalized, so the pipeline focuses on using the already processed data to control the prosthetic arm. This streamlined pipeline reduces computational complexity and speeds up the response time since the signal-processing steps have already been completed. Both pipelines are designed to convert sEMG data into usable data that we can use to train machine learning models that drive the prosthetic's movements, with the first pipeline offering a more comprehensive solution for real-time signal acquisition and processing.<br>
                        <br>In the end, to keep things simpler and more computationally efficient, we simply used the preprocessed signals in our final data collection and model training efforts.&nbsp;
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <img class="u-image u-image-contain u-image-default u-image-1" src="images/image22.png" alt="" data-image-width="1177" data-image-height="418">
      </div>
    </section>
    <section class="u-clearfix u-section-4" id="carousel_1968">
      <div class="u-clearfix u-sheet u-sheet-1">
        <div class="custom-expanded data-layout-selected u-clearfix u-gutter-10 u-layout-wrap u-layout-wrap-1">
          <div class="u-layout">
            <div class="u-layout-row">
              <div class="u-size-60">
                <div class="u-layout-col">
                  <div class="u-align-left u-border-no-bottom u-border-no-left u-border-no-right u-border-no-top u-container-align-left u-container-style u-layout-cell u-right-cell u-size-60 u-layout-cell-1">
                    <div class="u-container-layout u-container-layout-1">
                      <h2 class="u-align-left u-custom-font u-font-merriweather u-text u-text-1"> Data Collection<br>
                        <span style="font-size: 1.125rem;"> Our second step was to have a way to collect all of our data:</span>
                        <br>
                      </h2>
                      <p class="u-align-left u-text u-text-2"> To collect data, first we had to ensure electrical and firmware systems were in place, thus everything connected the way it should be to the arduino (which for data collection would simply be connecting the sensors to power (5v) and ground, along with connecting the signal output to the analog pins on our arduino. From there we can simply connect it to our computer and open up the script ArduinoDataCollection.ino (see github) which simply recognizes pin connections and outputs them to serial so that our data can be read by the script. <br>
                        <br>Next we must create a python script which reads our data and outputs it to a csv file. To do this we first set our serial port the arduino is connected to, the baudrate of the arduino, and the directory (folder) we want to save our csv files to. We can then create a list of all the gestures we want to collect data for. Ensure to include a gesture where your hand is relaxed to ensure the model has something to default to when it doesn’t detect a gesture. You can also set a duration and number of repetitions. In our case, we found a 0.5 second duration of data collection, along with doing 100 repetitions of the gesture was the sweet spot to ensure our models were accurate and fast. We then create a function to read our serial data which is given to us by the arduino, and outputs it to a csv file, with the first column being timestamps and the next columns being dependent on how many sensors you decided to collect data from. To make this easier for the user, our script has instructions appear on the users’ terminal as they collect data as seen below:&nbsp;&nbsp;<br>
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <img class="u-image u-image-default u-image-1" src="images/image32.png" alt="" data-image-width="1830" data-image-height="641">
      </div>
    </section>
    <section class="u-align-center u-clearfix u-container-align-center u-section-5" id="carousel_6727">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <h1 class="u-align-center u-custom-font u-font-merriweather u-text u-text-1">Machine Learning Steps:<br>
          <span style="font-size: 1.125rem;">The third step in our software pipeline is training a model on the data we collected to have it identify which signals correspond to which gesture: </span>
          <br>
        </h1>
      </div>
    </section>
    <section class="u-align-center u-clearfix u-container-align-center u-grey-10 u-section-6" id="carousel_67b4">
      <div class="u-clearfix u-sheet u-sheet-1">
        <div class="u-list u-list-1">
          <div class="u-repeater u-repeater-1">
            <div class="u-container-style u-list-item u-opacity u-opacity-20 u-repeater-item u-white u-list-item-1">
              <div class="u-container-layout u-similar-container u-valign-middle u-container-layout-1">
                <h2 class="u-text u-text-default u-text-1">1</h2>
                <p class="u-text u-text-grey-40 u-text-2"><b style="">Loading and Reading EMG data:&nbsp;</b>
                  <br> The sEMG signals are collected from CSV files generated by five sensors attached to the user’s arm. These signals are preprocessed by segmenting them into fixed-size windows with a certain number of samples per window. Each window is labeled with a gesture based on its file name. 
                </p>
              </div>
            </div>
            <div class="u-container-style u-custom-item u-list-item u-repeater-item u-white u-list-item-2">
              <div class="u-container-layout u-similar-container u-valign-middle u-container-layout-2">
                <h2 class="u-text u-text-default u-text-3">2</h2>
                <p class="u-text u-text-grey-40 u-text-4">
                  <span style="font-weight: 700;">Split Data:&nbsp;<br>
                  </span> The preprocessed dataset is divided into three parts:<br>
                  <br>Training Set: Used to teach the model the relationship between EMG signals and gestures.<br>
                  <br>Validation Set: Used to validate the model during training and prevent overfitting. <br>
                  <br>Test Set: Used to evaluate the model’s final performance and generalization to unseen data. This ensures that the model did not overfit to the training or validation sets.&nbsp;<br>
                </p>
              </div>
            </div>
            <div class="u-container-style u-list-item u-opacity u-opacity-20 u-repeater-item u-white u-list-item-3">
              <div class="u-container-layout u-similar-container u-valign-middle u-container-layout-3">
                <h2 class="u-text u-text-default u-text-5">3<br>
                </h2>
                <p class="u-text u-text-grey-40 u-text-6">
                  <span style="font-weight: 700;">Define Model Architecture:&nbsp;<br>
                  </span> This step defines the architecture of the neural network, which is specifically a Convolutional Neural Network (CNN). This type of model is effective for time-series data like EMG signals. <br>
                  <br>The structure consists of multiple convolutional layers, including batch normalization, pooling, and fully connected layers. A batch normalization layer normalizes the inputs of each layer by scaling and shifting them to improve training speed, stability, and model performance. A pooling layer reduces the spatial dimensions of the input, helping to decrease computational load and prevent overfitting while retaining important features. Finally, the fully connected layer connects every neuron from the previous layer to each neuron in the current layer, enabling the model to make final predictions based on learned features. <br>
                  <br>These layers allow the CNN to learn meaningful patterns in the data, allowing it to output a prediction of one of the gestures. This is where a lot of our time was spent tuning parameters which gave us the best possible model using our data.<br>
                </p>
              </div>
            </div>
            <div class="u-container-style u-list-item u-repeater-item u-white u-list-item-4">
              <div class="u-container-layout u-similar-container u-valign-middle u-container-layout-4">
                <h2 class="u-text u-text-default u-text-7">4<br>
                </h2>
                <p class="u-text u-text-grey-40 u-text-8">
                  <span style="font-weight: 700;">Setup Model:&nbsp;<br>
                  </span>
                  <br> Loss Function: The loss function used is cross-entropy loss, which measures how well the model predicts the correct gestures by finding the difference between the predicted probabilities and the actual gesture labels while penalizing incorrect predictions more heavily.<br>
                  <br>Optimizer: The optimizer AdamW is used to adjust the model’s weights during training by minimizing the loss function, this ensures efficient and stable convergence.&nbsp;
                </p>
              </div>
            </div>
            <div class="u-container-style u-list-item u-opacity u-opacity-20 u-repeater-item u-white u-list-item-5">
              <div class="u-container-layout u-similar-container u-valign-middle u-container-layout-5">
                <h2 class="u-text u-text-default u-text-9">5 </h2>
                <p class="u-text u-text-grey-40 u-text-10">
                  <span style="font-weight: 700;">Train Model:&nbsp;<br>
                  </span>The model is trained by iteratively feeding training data through the network, calculating loss, and updating the weights accordingly. Model performance is evaluated after each epoch, which is a single complete pass through the entire training dataset, where the model processes all data points at least once. The model is evaluated on the validation set to avoid overfitting. Validation loss and accuracy is plotted during training to visualize trends and understand model performance.&nbsp;<br>
                </p>
              </div>
            </div>
            <div class="u-container-style u-list-item u-repeater-item u-white u-list-item-6">
              <div class="u-container-layout u-similar-container u-valign-middle u-container-layout-6">
                <h2 class="u-text u-text-default u-text-11">6 </h2>
                <p class="u-text u-text-grey-40 u-text-12">
                  <span style="font-weight: 700;">Evaluate Model:&nbsp;<br>
                  </span>After training, the model is evaluated using the test set to measure its ability to generalize to new data. The test accuracy reflects how well the model can interpret gestures in real-world scenarios.<br>
                </p>
              </div>
            </div>
            <div class="u-container-style u-list-item u-opacity u-opacity-80 u-repeater-item u-white u-list-item-7">
              <div class="u-container-layout u-similar-container u-valign-middle u-container-layout-7">
                <h2 class="u-text u-text-default u-text-13">7 </h2>
                <p class="u-text u-text-grey-40 u-text-14">
                  <span style="font-weight: 700;">Save and Load Model:&nbsp;<br>
                  </span>Once performance is satisfactory, the trained model is saved for deployment. This allows it to later be loaded and integrated with hardware using a Raspberry Pi. The Raspberry Pi acts as the "brain" in this case with the Arduino sending over the sEMG data collected from the sensors on your arm, the Raspberry Pi predicts what gesture it is, and it sends it back to the Arduino which in turn moves motors to replicate the gesture on a hand.&nbsp;<br>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="u-clearfix u-container-align-center u-grey-5 u-section-7" id="sec-4ed7">
      <div class="u-clearfix u-sheet u-sheet-1">
        <div class="data-layout-selected u-clearfix u-expanded-width u-gutter-10 u-layout-wrap u-layout-wrap-1">
          <div class="u-layout" style="">
            <div class="u-layout-row" style="">
              <div class="u-container-style u-layout-cell u-left-cell u-size-30 u-size-xs-60 u-layout-cell-1" src="">
                <div class="u-container-layout u-container-layout-1">
                  <h2 class="u-align-center u-custom-font u-font-merriweather u-text u-text-1">Plots</h2>
                  <p class="u-align-center u-text u-text-2"> To the right is an example of a plot we get after training our model. As you can see, as the model trains its “Loss” (also known as error) decreases steadily, thus the model is learning. The bottom plot is of our validation accuracy which in this case steadily rose until we got to around 94% accuracy.</p>
                </div>
              </div>
              <div class="u-align-center u-container-align-center u-container-style u-image u-layout-cell u-right-cell u-size-30 u-size-xs-60 u-image-1" src="" data-image-width="495" data-image-height="753">
                <div class="u-container-layout u-container-layout-2" src=""></div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="u-clearfix u-section-8" id="carousel_98d2">
      <div class="u-clearfix u-sheet u-sheet-1">
        <div class="custom-expanded data-layout-selected u-clearfix u-gutter-10 u-layout-wrap u-layout-wrap-1">
          <div class="u-layout">
            <div class="u-layout-row">
              <div class="u-size-60">
                <div class="u-layout-col">
                  <div class="u-align-left u-border-no-bottom u-border-no-left u-border-no-right u-border-no-top u-container-align-left u-container-style u-layout-cell u-right-cell u-size-60 u-layout-cell-1">
                    <div class="u-container-layout u-container-layout-1">
                      <h2 class="u-align-left u-custom-font u-font-merriweather u-text u-text-1"> Live Detection of Gestures<span style="font-size: 1.125rem;">
                          <br>Lastly, we want to create a script which can detect the gestures you create in real time:
                        </span>
                        <br>
                      </h2>
                      <p class="u-align-left u-text u-text-2"> Finally, we must create a script that can communicate with our firmware to move motors based on the gesture it predicts. To streamline our setup, this is the code which goes on our Raspberry Pi and acts as the brain of our system, taking in sensor values from our firmware and outputting the prediction to our firmware to control our motors. <br>
                        <br>To create this script, we follow a similar starting process to our data collection script. We set our serial port, baud rate, and window size (which is how much data we’re reading before making a prediction). We then take the CNN architecture we described while training our model and add it to the script. In this case, ours was:&nbsp; &nbsp;<br>
                      </p>
                      <img class="u-image u-image-default u-image-1" src="images/image36.png" alt="" data-image-width="598" data-image-height="768">
                      <p class="u-align-left u-text u-text-3"> Next we can load the model which we saved from training our model. The one which worked best for us was 9gesture_model.pth, as it had around a 94% accuracy when we trained it. We originally trained a model on 22 gestures but decided to scale back as our accuracy (Just under 90%) wasn’t as high as we hoped it would be. We then set a dictionary of all the values we want to use. For us we included a rock gesture, paper gesture, scissor gesture, phone call gesture, thumb only gesture, pinky only gesture, a three gesture, a four gesture, a spiderman gesture, and a completely relaxed gesture. <br>
                        <br>After this, we can create a function which gets emg data from the arduino, storing it in an array.&nbsp;&nbsp;<br>
                      </p>
                      <img class="u-image u-image-default u-image-2" src="images/image35.png" alt="" data-image-width="780" data-image-height="376">
                      <p class="u-align-left u-text u-text-4"> We can then create a function which uses the loaded model to make a prediction based on the data it collected.&nbsp;&nbsp;<br>
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="u-clearfix u-section-9" id="sec-dc62">
      <div class="u-clearfix u-sheet u-sheet-1">
        <img class="u-image u-image-default u-image-1" src="images/image37.png" alt="" data-image-width="794" data-image-height="273">
        <p class="u-text u-text-1"> Lastly, we create a function which sends our prediction over to the the arduino so that the firmware code can move motors based on what the gesture was.</p>
        <img class="u-image u-image-default u-image-2" src="images/image38.png" alt="" data-image-width="888" data-image-height="275">
        <p class="u-text u-text-2"> We then loop through this until we wish to end the session.&nbsp;</p>
      </div>
    </section>
    <section class="u-align-center u-clearfix u-container-align-center u-section-10" id="sec-2253">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <h2 class="u-align-center u-custom-font u-font-merriweather u-text u-text-default u-text-1">And all of it coming together! </h2>
        <p class="u-align-center u-text u-text-2">Below is an explanation of everything software and how it was done!</p>
        <div class="u-video u-video-1">
          <div class="embed-responsive embed-responsive-1">
            <iframe style="position: absolute;top: 0;left: 0;width: 100%;height: 100%;" class="embed-responsive-item" src="https://www.youtube.com/embed/rtmoLfitXMg?mute=0&amp;showinfo=0&amp;controls=0&amp;start=0" frameborder="0" allowfullscreen=""></iframe>
          </div>
        </div>
      </div>
    </section>
    <section class="u-clearfix u-section-11" id="sec-e926">
      <div class="u-clearfix u-sheet u-sheet-1">
        <h1 class="u-align-center u-custom-font u-font-courier-new u-text u-text-1">You can view our code here on our public GitHub Repository!</h1>
      </div>
    </section>
    <section class="u-clearfix u-section-12" id="sec-9d24">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <a href="https://github.com/mfox0914/PIE_Final_Project_CyberFlex" class="u-align-center u-border-0 u-btn u-btn-round u-button-style u-custom-font u-font-merriweather u-hover-feature u-hover-palette-1-light-1 u-palette-1-base u-radius u-btn-1">Source Code </a>
      </div>
      
    </section>
    
    
    
    <footer class="u-align-center u-clearfix u-container-align-center u-footer u-grey-80 u-footer" id="sec-ab67"><div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <p class="u-custom-font u-font-montserrat u-small-text u-text u-text-variant u-text-1">ABOUT THIS SITE:&nbsp;<br>A Principles of Integrated Engineering Final Project by Esther Aduamah, Maya Adelman, Michaela Fox, Darian Jimenez, and Suwanee Li!
        </p>
      </div></footer>
    <section class="u-backlink u-clearfix u-grey-80">
      <p class="u-text">
        <span>This site was created with the </span>
        <a class="u-link" href="https://nicepage.com/" target="_blank" rel="nofollow">
          <span>Nicepage</span>
        </a>
      </p>
    </section>
  
</body></html>